{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please install the yellowbrick package first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "sns.set_palette('husl')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, QuantileTransformer, MinMaxScaler, Normalizer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer, MissingIndicator\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from math import isclose\n",
    "\n",
    "from yellowbrick.cluster import SilhouetteVisualizer, KElbowVisualizer, silhouette_visualizer\n",
    "from yellowbrick.cluster.icdm import InterclusterDistance\n",
    "from yellowbrick.datasets import load_nfl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %logstop\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Discription\n",
    "\n",
    "- To obtain the optimal clustering algorithm for unsupervised learning of water molecules. \n",
    "\n",
    "- The water.csv file contains 1001 snapshots of 250 water molecules. i.e. each line has coordinates of 750 atoms (making total 2250 columns) and 2251st column is called flag which is not relevant in the beginning.\n",
    "\n",
    "- Since these are coordinates, the feature vector of dim 2250 can be reshaped into 750 X 3 matrix.\n",
    "\n",
    "- This essentially makes it a 750 dimensional array of 3-d vectors.\n",
    "\n",
    "- We will take the first vector and subtract it from all the vectors in the snapshot (reason expLained later). The feature vectors will be reshaped back to vector of dim 2250.\n",
    "\n",
    "- Clustering algorithm will be run on 1001 feature vectors of dim 2250. We will perform elbow and silhouette analysis and identify distinct clusters in 1001 snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File water.csv does not exist: 'water.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e088244b9bcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load Training Data From Disc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"water.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File water.csv does not exist: 'water.csv'"
     ]
    }
   ],
   "source": [
    "# Load Training Data From Disc\n",
    "\n",
    "data = pd.read_csv(\"water.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, I'll take a brief look at the data for the purpose of exploratory analysis and overview data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are there any Missing Data?\n",
    "\n",
    "- An important question to ask is how many observations, if any, have missing data and what proportion of entire set this represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proportion of missing values\")\n",
    "print(data.isnull().sum().sum()*100/len(data))\n",
    "\n",
    "print(\"Proportion of missing values\")\n",
    "print(data.isna().sum().sum()*100/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data has no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice Out The Flag Column\n",
    "\n",
    "- Since the 2251st column are the flag labels, they will be dropped from the dataframe and set as $Flags$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = data.iloc[:,2250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[2250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the Feature Space\n",
    "\n",
    "- The new DataFrame has 2250 features for 1001 snapshots\n",
    "\n",
    "- The objective is to reshape the feature space to a $750$ $X$ $3$ equavalent to a dimension corresposing to $Atoms$ $X$ $Coordinates$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe into an array to allow easy reshaping\n",
    "\n",
    "X = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape feature vectors into 750 X 3 dimensions\n",
    "\n",
    "X = X.reshape((1001, 750, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the new configuration, the shape $1001$ $X$ $750$ $X$ $3$ is equivalent to $Snapshots$ $X$ $Atoms$ $X$ $Coordinates$\n",
    "\n",
    "- Now, I'll subtract the first coordinates from all coordinates in order to create a reference coordinate.\n",
    "\n",
    "- This reference cooridinate will have values $0$, $0$, $0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new feature matrix as 3d matrix of shape (1001, 750, 3), for feature transformation\n",
    "\n",
    "X_new = np.zeros((X.shape[0], X.shape[1], X.shape[2]))\n",
    "y_new = np.zeros((X.shape[0], X.shape[1], X.shape[2]))\n",
    "\n",
    "# loop through all 1001 snapshots (i.e slices in the 3d matrix)\n",
    "for i in range(0, len(X)):\n",
    "    # Subtract coordinates in row 0 from coordinates in all other rows\n",
    "    X_new[i, :, :] = X[i][:, :] - X[i][0, :]\n",
    "    y_new[i, :, :] = flags[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now, we reshape the feature vectors into a vector or dimensions 2250 for each of the 1001 snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape back into (1001, 2250) matrix\n",
    "\n",
    "X = X_new.reshape((1001, 2250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For training pruposes, we require some kind of transformation that will covert the feature space into a more relevant feature space for the clustering algorithm.\n",
    "\n",
    "- To make this transformation, we consider the irreducible representation of the symmetry group of the raw data. \n",
    "\n",
    "- Examples of such symmetry groups for the vector coordinates being delt with here are\n",
    "\n",
    "$$Rotation$$\n",
    "\n",
    "$$Translation$$\n",
    "\n",
    "- These operations when performed on the data sets should maintain invariance of the problem.\n",
    "\n",
    "- However, we have performed such transformation in prior steps by subtracting the first vector from all other vectors in the snapshot in the $750$ $X$ $3$ representation of the feature vectors. \n",
    "\n",
    "- This procedure only creates a reference cooridnate ($0$, $0$, $0$) that doesn't change necessarily the physics.\n",
    "\n",
    "- Coupled with the fact that each of the feature vectors in the $2250$ dimensional feature space are all $vector$ $coordinates$, which have the physical measurements units, no further transformations need to be done. \n",
    "\n",
    "- This is further validated by the fact that the inherent distance measures$-$used to form clusters$-$use $vector$ $coordinates$ in the $Euclidean$ $space$. Since the the data already has such a representation, there's no pressing need for further transformation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The feature space has an extremely high dimensionality of 2250 feature vectors. One can therefore intuitively note that obtaining distance measures at such high dimensions is notoriously difficult.  \n",
    "\n",
    "- More so, the distance between the data points and the origin of the coordinate system grows as a square root of the number of dimensions D, which will in turn lead to divergence of the the Euclidean distance metric employed by most clustering algorithms of interest.\n",
    "\n",
    "- Since all of these feature vectors may not be necessary for explaining the overall variance (underlying structure)for individual snapshots, it is imporotant to investigate if a reduced set of feature vectors can capture ~$100$% of the overall data structure. \n",
    "\n",
    "- This will be done using $Principal$ $Component$ $Analysis$ because the algorithm inherently supports unserpeprvised learning , which is of particular interest in this question. \n",
    "\n",
    "- We will call the $Decomposition.PCA()$ method and investe the $cummulative$ $explained$ $variance$ $ratio$ as a function of number of components. \n",
    "\n",
    "- Since this automatically sorts the components in order of increasing variance, the first $n$$-$$components$ that results in ~$100$% of the $cummulative$ $explained$ $variance$ $ratio$ will be reserved as the set of new feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f5cf8623de18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#call PCA method and fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# plot cummulative variance (eigenvalues) explained by the first 150 eigenvectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#call PCA method and fit\n",
    "pca = PCA().fit(X)\n",
    "\n",
    "# plot cummulative variance (eigenvalues) explained by the first 150 eigenvectors\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)[0:151])\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The curve above gives the cumulative explained variance, which measures how well PCA preserves the content of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This curve quantifies how much of the total, 2250-dimensional variance is contained within the first $N$ $components$. For example, we see that with the snapshoots, the first 10 components contain approximately ~$93$% of the variance, while you need around 150 components to describe close to $100$% of the variance.\n",
    "\n",
    "- Therfore, we see that a 150-dimensional projection retains a lot of information (as measured by the explained variance) and that we’d need about 150 components to retain ~$100$% of the variance.\n",
    "\n",
    "- $Why$ $Is$ $The$ $150$$-$$dim$ $vector$ $a$ $reasonable$ $size$ $for$ $the$ $new$ $feature$ $space$? Because the size of the reduced feature space is markedly smaller than the size of the raw feature space, while still achieving ~$100$% explained variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Data Unto Lower (150-) Dimensional Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d58e2e59eec2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_projected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#check new shape of reduced feature vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# call PCA method and project raw data into new eigenbasis\n",
    "pca = PCA(n_components=150)\n",
    "\n",
    "X_projected = pca.fit_transform(X) \n",
    "\n",
    "#check new shape of reduced feature vectors\n",
    "print(X_projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering on Reduced Data\n",
    "\n",
    "- Here I compare performance of 3 popular clustering algorithms on the data set;\n",
    "\n",
    "$$Kmeans$$\n",
    "\n",
    "$$AgglomerativeClustering$$\n",
    "\n",
    "$$DBSCAN$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans Algorithm -- Centroid Based Clustering\n",
    "\n",
    "- The $K$-$Means$ algorithm is tested because it is perhaps the simplest algorithm in the group of $Centroid$ $Based$ $Clustering$ algorithms\n",
    "\n",
    "- A particular disavantage of the $Centroid$ $Based$ $Clustering$ algorithms is that they require the number of clusters to be specified a-priori which is hard to optimize due to inapplicability of cross-validation for clustering\n",
    "\n",
    "- Here I will vizualize $Silhouette$ $Curves$ for a range of cluster numbers in combinations with the hyperparamters specifying methods for initialization, so as to obtain the optimal number of clusters and initialization method.\n",
    "\n",
    "- The initialization methods considered are; \n",
    "\n",
    "    - \" ‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. \"\n",
    "\n",
    "    - \" ‘random’: choose n_clusters observations (rows) at random from data for the initial centroids \"  source https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_projected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3d9d73f50262>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# save labels to 3d array for  init_method, n_cluster combinations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0my_kmeans_pr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_projected\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_projected' is not defined"
     ]
    }
   ],
   "source": [
    "#initialize number of clusters as a list for looping\n",
    "n_clusters = list(np.arange(2, 6))\n",
    "\n",
    "#list for methods of initialization to be considered \n",
    "init_method = [\"k-means++\", \"random\"]\n",
    "\n",
    "# dataframe for errors for init_method, n_cluster combinations\n",
    "kmeans_error = pd.DataFrame(columns=[\"k-means++\", \"random\"], index=n_clusters, dtype=float)\n",
    "\n",
    "# save labels to 3d array for  init_method, n_cluster combinations\n",
    "y_kmeans_pr = np.zeros((len(n_clusters), X_projected.shape[0], len(init_method)))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(n_clusters), ncols=len(init_method), figsize=(14, 14), sharex=True, sharey=True)\n",
    "\n",
    "for n_idx, n in enumerate(n_clusters):\n",
    "    for method_idx, method in enumerate(init_method):\n",
    "        kmeans = KMeans(n_clusters=n, init=method)\n",
    "        kmeans.fit(X_projected)\n",
    "        y_kmeans = kmeans.predict(X_projected)\n",
    "        y_kmeans_pr[n_idx, :, method_idx] = y_kmeans\n",
    "        kmeans_error.loc[n, method] = kmeans.inertia_\n",
    "        # Creating SilhouetteVisualizer and assigning it to an axis\n",
    "        visualizer = SilhouetteVisualizer(kmeans, ax=axs[n_idx, method_idx], is_fitted='auto', legend=True)\n",
    "        visualizer.fit(X_projected)\n",
    "        # Setting subplot legends via axis\n",
    "        axs[n_idx, method_idx].legend([f'{n} clusters, Score={visualizer.silhouette_score_:.3f}'],\n",
    "                                            loc=\"upper right\", frameon=True, fontsize=10)\n",
    "        # Setting subplot x and y labels\n",
    "        axs[n_idx, method_idx].set(xlabel='silhouette coefficient values', ylabel='cluster label')\n",
    "\n",
    "axs[0, 0].set_title(\"k-means++\")\n",
    "axs[0, 1].set_title(\"random\")\n",
    "\n",
    "fig.text(0.5, 0.04, 'Method for Initialization', ha='center', fontsize=16)\n",
    "fig.text(0.04, 0.5, 'Number of Clusters', va='center', rotation='vertical',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "- Since silhouette coefficients near $+$$1$ indicate that the sample is far away from the neighboring clusters and a value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters, and negative values indicate that those samples might have been assigned to the wrong cluster, we can draw the following conlcusions from the $K$$-$$Means$ clustering algorthim;\n",
    "\n",
    "    - That, at first glance, $n$_$clusters$ values of $2$, $3$, $4$, and $5$ do not yield acceptible clustering of the data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots.\n",
    "    \n",
    "    - More so, the silhouette analysis doesn't yield significantly differing results for both initialization methods considered.\n",
    "    \n",
    "- A plausible reason for the failure of this algorithm could be it's lack of flexiblity. Since the feature space has a high demnsionality, it is possible that the clusters are noncircular, thus, such centroid-based clustering algorithm which can handle only clusters with spherical or ellipsoidal symmetry are likely to provide a very poor fit. \n",
    "\n",
    "- Although the clustering algorithm doesn't produce very good fit, it is seen from the plot that the number of clusters with the best score is $3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize K-Means Elbow\n",
    "\n",
    "- In order to expand the the range of $n$_$clusters$ considered in the algorithm, we will plot the elbow (inertia) curves for a wide range of values. This will also help to consolidate the deductions from the silhouette analysis. \n",
    "\n",
    "- To aid direct comparison, the metric used in this plot is the $Calinski$-$Harabasz$ $Index$ which, the ratio of within to between cluster dispersion. The dispersion is defined as the sum of distances squared. \n",
    "\n",
    "- Higher scores of this index reflect dense, well separated clusters, which gives a reasonable standard concept of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans()\n",
    "n_clusters_range = (2, 12)\n",
    "visualizer = KElbowVisualizer(kmeans_model, k=n_clusters_range, metric='calinski_harabasz', timings=False, locate_elbow=True)\n",
    "visualizer.fit(X_projected)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "- It is evident from the silhouette analysis and the elbow plot that the optimal number of clusters predicted by the  $K$$-$$Means$ clustering algorthim is $3$ since this results in the highest silhouette coefficient (nearest to +1) and $Calinski$-$Harabasz$ score, which by definition indicates highly dense clusters. \n",
    "\n",
    "\n",
    "- However, the analysis shows that n_clusters value of $3$ is still a bad pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots.\n",
    "\n",
    "- Therefore, this model will be trained with $n$_$clusters$ = $3$ but other clustering algorithms will have to be tested to see if they produce better results/fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitted Model\n",
    "\n",
    "- $K$-$means$ fittted with $n$_$clusters$ = $3$ and $init$_$method$ = $K$-$means++$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_projected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-72649eb9f025>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"k-means++\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_projected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_kmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_projected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_projected' is not defined"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3, init=\"k-means++\")\n",
    "kmeans.fit(X_projected)\n",
    "y_kmeans = kmeans.predict(X_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering -- Heirachical Clustering\n",
    "\n",
    "- A reason for the poor performance of the $K$-$Means$ algorithm could be the lack of flexibility in cluster shape coupled with the curse of dimensionality that could lead to the divergence of the Euclidean Distance.\n",
    "\n",
    "- Since $Heirachical$ $Clustering$ algorithms recursively merge pair of clusters that minimally increases a given linkage distance, it is possible that this resolves the sphere-like clustering therefore having fewer assumptions about the distribution of the data - the only requirement (which $K$-$Means$ also shares) is the distance metric calculated for each pair of clusters.. \n",
    "\n",
    "- Therefore, in order to have more control over the shape of cluster assignment, the Agglomerative Clustering algorithm is considered, since it doesnt restrict the cluster to be spherical.\n",
    "\n",
    "- Here I will vizualize $Silhouette$ $Curves$ for a range of cluster numbers as well as combinations of the hyperparamters specifying the affinity, so as to obtain the optimal number of clusters and distance metric.\n",
    "\n",
    "- The distance measures considered are; \n",
    "\n",
    "    - 'euclidean': Usual square distance between the two vectors (2 norm).\"\n",
    "\n",
    "    - 'manhattan':\tAbsolute distance between the two vectors (1 norm). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is a collapsed cell function for plotting. Please run the cell first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# yellowbrick.cluster.silhouette\n",
    "# Implements visualizers using the silhouette metric for cluster evaluation.\n",
    "#\n",
    "# Author:   Benjamin Bengfort\n",
    "# Author:   Rebecca Bilbro\n",
    "# Created:  Mon Mar 27 10:09:24 2017 -0400\n",
    "#\n",
    "# Copyright (C) 2017 The scikit-yb developers\n",
    "# For license information, see LICENSE.txt\n",
    "#\n",
    "# ID: silhouette.py [57b563b] benjamin@bengfort.com $\n",
    "\n",
    "\"\"\"\n",
    "Implements visualizers that use the silhouette metric for cluster evaluation.\n",
    "\"\"\"\n",
    "\n",
    "##########################################################################\n",
    "## Imports\n",
    "##########################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "from yellowbrick.utils import check_fitted\n",
    "from yellowbrick.style import resolve_colors\n",
    "from yellowbrick.cluster.base import ClusteringScoreVisualizer\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Silhouette Method for K Selection\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "class agc_SilhouetteVisualizer(ClusteringScoreVisualizer):\n",
    "    \"\"\"\n",
    "    The Silhouette Visualizer displays the silhouette coefficient for each\n",
    "    sample on a per-cluster basis, visually evaluating the density and\n",
    "    separation between clusters. The score is calculated by averaging the\n",
    "    silhouette coefficient for each sample, computed as the difference\n",
    "    between the average intra-cluster distance and the mean nearest-cluster\n",
    "    distance for each sample, normalized by the maximum value. This produces a\n",
    "    score between -1 and +1, where scores near +1 indicate high separation\n",
    "    and scores near -1 indicate that the samples may have been assigned to\n",
    "    the wrong cluster.\n",
    "\n",
    "    In SilhouetteVisualizer plots, clusters with higher scores have wider\n",
    "    silhouettes, but clusters that are less cohesive will fall short of the\n",
    "    average score across all clusters, which is plotted as a vertical dotted\n",
    "    red line.\n",
    "\n",
    "    This is particularly useful for determining cluster imbalance, or for\n",
    "    selecting a value for K by comparing multiple visualizers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : a Scikit-Learn clusterer\n",
    "        Should be an instance of a centroidal clustering algorithm (``KMeans``\n",
    "        or ``MiniBatchKMeans``). If the estimator is not fitted, it is fit when\n",
    "        the visualizer is fitted, unless otherwise specified by ``is_fitted``.\n",
    "\n",
    "    ax : matplotlib Axes, default: None\n",
    "        The axes to plot the figure on. If None is passed in the current axes\n",
    "        will be used (or generated if required).\n",
    "\n",
    "    colors : iterable or string, default: None\n",
    "        A collection of colors to use for each cluster group. If there are\n",
    "        fewer colors than cluster groups, colors will repeat. May also be a\n",
    "        Yellowbrick or matplotlib colormap string.\n",
    "\n",
    "    is_fitted : bool or str, default='auto'\n",
    "        Specify if the wrapped estimator is already fitted. If False, the\n",
    "        estimator will be fit when the visualizer is fit, otherwise, the\n",
    "        estimator will not be modified. If 'auto' (default), a helper method\n",
    "        will check if the estimator is fitted before fitting it again.\n",
    "\n",
    "    kwargs : dict\n",
    "        Keyword arguments that are passed to the base class and may influence\n",
    "        the visualization as defined in other Visualizers.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    silhouette_score_ : float\n",
    "        Mean Silhouette Coefficient for all samples. Computed via scikit-learn\n",
    "        `sklearn.metrics.silhouette_score`.\n",
    "\n",
    "    silhouette_samples_ : array, shape = [n_samples]\n",
    "        Silhouette Coefficient for each samples. Computed via scikit-learn\n",
    "        `sklearn.metrics.silhouette_samples`.\n",
    "\n",
    "    n_samples_ : integer\n",
    "        Number of total samples in the dataset (X.shape[0])\n",
    "\n",
    "    n_clusters_ : integer\n",
    "        Number of clusters (e.g. n_clusters or k value) passed to internal\n",
    "        scikit-learn model.\n",
    "\n",
    "    y_tick_pos_ : array of shape (n_clusters,)\n",
    "        The computed center positions of each cluster on the y-axis\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    >>> from yellowbrick.cluster import agc_SilhouetteVisualizer\n",
    "    >>> from sklearn.cluster import KMeans\n",
    "    >>> model = agc_SilhouetteVisualizer(KMeans(10))\n",
    "    >>> model.fit(X)\n",
    "    >>> model.show()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimator, ax=None, colors=None, is_fitted=\"auto\", **kwargs):\n",
    "\n",
    "        # Initialize the visualizer bases\n",
    "        super(agc_SilhouetteVisualizer, self).__init__(estimator, ax=ax, **kwargs)\n",
    "\n",
    "        # Visual Properties\n",
    "        # Use colors if it is given, otherwise attempt to use colormap which\n",
    "        # which will override colors. If neither is found, default to None.\n",
    "        # The colormap may yet still be found in resolve_colors\n",
    "        self.colors = colors\n",
    "        if \"colormap\" in kwargs:\n",
    "            self.colors = kwargs[\"colormap\"]\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Fits the model and generates the silhouette visualization.\n",
    "        \"\"\"\n",
    "        # TODO: decide to use this method or the score method to draw.\n",
    "        # NOTE: Probably this would be better in score, but the standard score\n",
    "        # is a little different and I'm not sure how it's used.\n",
    "\n",
    "        if not check_fitted(self.estimator, is_fitted_by=self.is_fitted):\n",
    "            # Fit the wrapped estimator\n",
    "            self.estimator.fit(X, y, **kwargs)\n",
    "\n",
    "        # Get the properties of the dataset\n",
    "        self.n_samples_ = X.shape[0]\n",
    "        self.n_clusters_ = self.estimator.n_clusters\n",
    "\n",
    "        # Compute the scores of the cluster\n",
    "        labels = self.estimator.fit_predict(X)\n",
    "        self.silhouette_score_ = silhouette_score(X, labels)\n",
    "        self.silhouette_samples_ = silhouette_samples(X, labels)\n",
    "\n",
    "        # Draw the silhouette figure\n",
    "        self.draw(labels)\n",
    "\n",
    "        # Return the estimator\n",
    "        return self\n",
    "\n",
    "\n",
    "    def draw(self, labels):\n",
    "        \"\"\"\n",
    "        Draw the silhouettes for each sample and the average score.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        labels : array-like\n",
    "            An array with the cluster label for each silhouette sample,\n",
    "            usually computed with ``predict()``. Labels are not stored on the\n",
    "            visualizer so that the figure can be redrawn with new data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Track the positions of the lines being drawn\n",
    "        y_lower = 10  # The bottom of the silhouette\n",
    "\n",
    "        # Get the colors from the various properties\n",
    "        color_kwargs = {\"n_colors\": self.n_clusters_}\n",
    "\n",
    "        if self.colors is None:\n",
    "            color_kwargs[\"colormap\"] = \"Set1\"\n",
    "        elif isinstance(self.colors, str):\n",
    "            color_kwargs[\"colormap\"] = self.colors\n",
    "        else:\n",
    "            color_kwargs[\"colors\"] = self.colors\n",
    "\n",
    "        colors = resolve_colors(**color_kwargs)\n",
    "\n",
    "        # For each cluster, plot the silhouette scores\n",
    "        self.y_tick_pos_ = []\n",
    "        for idx in range(self.n_clusters_):\n",
    "\n",
    "            # Collect silhouette scores for samples in the current cluster .\n",
    "            values = self.silhouette_samples_[labels == idx]\n",
    "            values.sort()\n",
    "\n",
    "            # Compute the size of the cluster and find upper limit\n",
    "            size = values.shape[0]\n",
    "            y_upper = y_lower + size\n",
    "\n",
    "            color = colors[idx]\n",
    "            self.ax.fill_betweenx(\n",
    "                np.arange(y_lower, y_upper),\n",
    "                0,\n",
    "                values,\n",
    "                facecolor=color,\n",
    "                edgecolor=color,\n",
    "                alpha=0.5,\n",
    "            )\n",
    "\n",
    "            # Collect the tick position for each cluster\n",
    "            self.y_tick_pos_.append(y_lower + 0.5 * size)\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        self.ax.axvline(\n",
    "            x=self.silhouette_score_,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            label=\"Average Silhouette Score\",\n",
    "        )\n",
    "\n",
    "        return self.ax\n",
    "\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        Prepare the figure for rendering by setting the title and adjusting\n",
    "        the limits on the axes, adding labels and a legend.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the title\n",
    "        self.set_title(\n",
    "            (\"Silhouette Plot of {} Clustering for {} Samples in {} Centers\").format(\n",
    "                self.name, self.n_samples_, self.n_clusters_\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Set the X and Y limits\n",
    "        # The silhouette coefficient can range from -1, 1;\n",
    "        # but here we scale the plot according to our visualizations\n",
    "\n",
    "        # l_xlim and u_xlim are lower and upper limits of the x-axis,\n",
    "        # set according to our calculated max and min score with necessary padding\n",
    "        l_xlim = max(-1, min(-0.1, round(min(self.silhouette_samples_) - 0.1, 1)))\n",
    "        u_xlim = min(1, round(max(self.silhouette_samples_) + 0.1, 1))\n",
    "        self.ax.set_xlim([l_xlim, u_xlim])\n",
    "\n",
    "        # The (n_clusters_+1)*10 is for inserting blank space between\n",
    "        # silhouette plots of individual clusters, to demarcate them clearly.\n",
    "        self.ax.set_ylim([0, self.n_samples_ + (self.n_clusters_ + 1) * 10])\n",
    "\n",
    "        # Set the x and y labels\n",
    "        self.ax.set_xlabel(\"silhouette coefficient values\")\n",
    "        self.ax.set_ylabel(\"cluster label\")\n",
    "\n",
    "        # Set the ticks on the axis object.\n",
    "        self.ax.set_yticks(self.y_tick_pos_)\n",
    "        self.ax.set_yticklabels(str(idx) for idx in range(self.n_clusters_))\n",
    "        # Set the ticks at multiples of 0.1\n",
    "        self.ax.xaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "\n",
    "        # Show legend (Average Silhouette Score axis)\n",
    "        self.ax.legend(loc=\"best\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_projected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-05420a330714>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# save labels to 3d array for  init_method, n_cluster combinations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0my_agc_pr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_projected\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance_measure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_projected' is not defined"
     ]
    }
   ],
   "source": [
    "#initialize number of clusters as a list for looping\n",
    "n_clusters = list(np.arange(2, 6))\n",
    "\n",
    "#list for distance measures to be considered \n",
    "distance_measure = [\"euclidean\", \"manhattan\"]\n",
    "\n",
    "# save labels to 3d array for  init_method, n_cluster combinations\n",
    "y_agc_pr = np.zeros((len(n_clusters), X_projected.shape[0], 2))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(n_clusters), ncols=len(distance_measure), figsize=(14, 14), sharex=True, sharey=True)\n",
    "\n",
    "# ignore minor warnings like deprecated attributes in new_version of sklearn\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    for n_idx, n in enumerate(n_clusters):\n",
    "        for dist_measure_idx, metric in enumerate(distance_measure):      \n",
    "            agc = AgglomerativeClustering(n_clusters=n, affinity=metric, linkage=\"single\")\n",
    "            y_agc = agc.fit_predict(X_projected)\n",
    "            y_agc_pr[n_idx, :, dist_measure_idx] = y_agc\n",
    "            # Creating SilhouetteVisualizer and assigning it to an axis\n",
    "            visualizer = agc_SilhouetteVisualizer(agc, ax=axs[n_idx, dist_measure_idx], is_fitted=True, legend=True)\n",
    "            visualizer.fit(X_projected)\n",
    "            # Setting subplot legends via axis\n",
    "            axs[n_idx, dist_measure_idx].legend([f'{n} clusters, Score={visualizer.silhouette_score_:.3f}'],\n",
    "                                                loc=\"upper right\", frameon=True, fontsize=10)\n",
    "            # Setting subplot x and y labels\n",
    "            axs[n_idx, dist_measure_idx].set(xlabel='silhouette coefficient values', ylabel='cluster label')\n",
    "\n",
    "axs[0, 0].set_title(\"euclidean\")\n",
    "axs[0, 1].set_title(\"manhattan\")\n",
    "\n",
    "fig.text(0.5, 0.04, 'Affinity', ha='center', fontsize=16)\n",
    "fig.text(0.04, 0.5, 'Number of Clusters', va='center', rotation='vertical', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "- We can draw the following conlcusions from the $Agglomerative$ $Clustering$ algorthim;\n",
    "\n",
    "    - That, as observed in $K$-$Means$ clustering, at first glance, $n$_$clusters$ values of 2, 3, 4, and 5 do not yield acceptible clustering of the data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots.\n",
    "    \n",
    "    - However, the silhouette analysis yields significantly differing results for both distance measures considered.\n",
    "    \n",
    "    - Since the $euclidean$ distance measure does not yield negative $silhouette$ $coefficients$, this is the preferred distance measure for this algorithm. \n",
    "    \n",
    "- A plausible reason for the failure of this algorithm could be it's sensitivity to noise in the data, which appears in the recursive merging procedure.\n",
    "\n",
    "- Although the clustering algorithm doesn't produce very good fit, it is seen from the plot that the number of clusters with the best score is $3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Agglomerative Clustering Elbow\n",
    "\n",
    "- In order to expand the the range of $n$_$clusters$ considered in the algorithm, we will plot the elbow (inertia) curves for a wide range of values. This will also help to consolidate the deductions from the silhouette analysis. \n",
    "\n",
    "- To aid direct comparison, the metric used in this plot is the $Calinski$-$Harabasz$ $Index$ which, the ratio of within to between cluster dispersion. The dispersion is defined as the sum of distances squared. \n",
    "\n",
    "- Higher scores of this index reflect dense, well separated clusters, which gives a reasonable standard concept of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agc_model = AgglomerativeClustering(affinity=\"euclidean\", linkage=\"single\")\n",
    "n_clusters_range = (2, 12)\n",
    "visualizer = KElbowVisualizer(agc_model, k=n_clusters_range, metric='calinski_harabasz', timings=False, locate_elbow=True)\n",
    "visualizer.fit(X_projected)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "- It is evident from the silhouette analysis and the elbow plot that the optimal number of clusters predicted by the  $AgglomerativeClustering$ algorthim is $3$ since this results in the highest silhouette coefficient (nearest to +1) and $Calinski$-$Harabasz$ score, which by definition indicates highly dense clusters. \n",
    "\n",
    "- However, the analysis shows that n_clusters value of $3$ is still a bad pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots.\n",
    "\n",
    "- Therefore, this model will be trained with $n$_$clusters$ = $3$ but other clustering algorithms will have to be tested to see if they produce better results/fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitted Model\n",
    "\n",
    "- $Agglomerative$ $Clustering$ fittted with $n$_$clusters$ = $3$ and $affinity$ = $euclidean$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_projected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d60e2bf38b3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0magc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maffinity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"euclidean\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"single\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_agc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_projected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_projected' is not defined"
     ]
    }
   ],
   "source": [
    "agc = AgglomerativeClustering(n_clusters=3, affinity=\"euclidean\", linkage=\"single\")\n",
    "y_agc = agc.fit_predict(X_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN -- Density-Based Clustering\n",
    "\n",
    "- Despite its advantages over the $K$-$Means$ algorithm, $AgglomerativeClustering$, is particularly known to be sensitive to noise in the data--although this is not expected to significantly impact performance since some of the noise has been inadvertently fitered out by PCA.\n",
    "\n",
    "- In addition, the number of clusters still has to be implicitly specified a-priori via the “resolution” hyperparameters. Changing the hyperparameters can easily result in fewer or more clusters which is somehow arbitrary and hence quite unsatisfactory as there is no obvious way to define an objective function for automated tuning of the hyperparameters.\n",
    "\n",
    "- Out of all clustering algorithms, only Density-based (DBSCAN) allows clustering without specifying the number of clusters a-priori and can identify outliers as noise, instead of classifying them into a cluster, an area $AgglomerativeClustering$ crumbles.\n",
    "\n",
    "- More so, such density based clustering algorithms are known indifferent to the shape of clusters and is also robust with respect to clusters with different density. Thus, the $DBSCAN$ algorithm will be tested.\n",
    "\n",
    "- In this case the hyperparameters tuned are $min$_$samples$ and $eps$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is a collapsed cell function for plotting. Please run the cell first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# yellowbrick.cluster.silhouette\n",
    "# Implements visualizers using the silhouette metric for cluster evaluation.\n",
    "#\n",
    "# Author:   Benjamin Bengfort\n",
    "# Author:   Rebecca Bilbro\n",
    "# Created:  Mon Mar 27 10:09:24 2017 -0400\n",
    "#\n",
    "# Copyright (C) 2017 The scikit-yb developers\n",
    "# For license information, see LICENSE.txt\n",
    "#\n",
    "# ID: silhouette.py [57b563b] benjamin@bengfort.com $\n",
    "\n",
    "\"\"\"\n",
    "Implements visualizers that use the silhouette metric for cluster evaluation.\n",
    "\"\"\"\n",
    "\n",
    "##########################################################################\n",
    "## Imports\n",
    "##########################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "from yellowbrick.utils import check_fitted\n",
    "from yellowbrick.style import resolve_colors\n",
    "from yellowbrick.cluster.base import ClusteringScoreVisualizer\n",
    "\n",
    "## Packages for export\n",
    "\n",
    "##########################################################################\n",
    "## Silhouette Method for K Selection\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "class db_SilhouetteVisualizer(ClusteringScoreVisualizer):\n",
    "    \"\"\"\n",
    "    The Silhouette Visualizer displays the silhouette coefficient for each\n",
    "    sample on a per-cluster basis, visually evaluating the density and\n",
    "    separation between clusters. The score is calculated by averaging the\n",
    "    silhouette coefficient for each sample, computed as the difference\n",
    "    between the average intra-cluster distance and the mean nearest-cluster\n",
    "    distance for each sample, normalized by the maximum value. This produces a\n",
    "    score between -1 and +1, where scores near +1 indicate high separation\n",
    "    and scores near -1 indicate that the samples may have been assigned to\n",
    "    the wrong cluster.\n",
    "\n",
    "    In SilhouetteVisualizer plots, clusters with higher scores have wider\n",
    "    silhouettes, but clusters that are less cohesive will fall short of the\n",
    "    average score across all clusters, which is plotted as a vertical dotted\n",
    "    red line.\n",
    "\n",
    "    This is particularly useful for determining cluster imbalance, or for\n",
    "    selecting a value for K by comparing multiple visualizers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : a Scikit-Learn clusterer\n",
    "        Should be an instance of a centroidal clustering algorithm (``KMeans``\n",
    "        or ``MiniBatchKMeans``). If the estimator is not fitted, it is fit when\n",
    "        the visualizer is fitted, unless otherwise specified by ``is_fitted``.\n",
    "\n",
    "    ax : matplotlib Axes, default: None\n",
    "        The axes to plot the figure on. If None is passed in the current axes\n",
    "        will be used (or generated if required).\n",
    "\n",
    "    colors : iterable or string, default: None\n",
    "        A collection of colors to use for each cluster group. If there are\n",
    "        fewer colors than cluster groups, colors will repeat. May also be a\n",
    "        Yellowbrick or matplotlib colormap string.\n",
    "\n",
    "    is_fitted : bool or str, default='auto'\n",
    "        Specify if the wrapped estimator is already fitted. If False, the\n",
    "        estimator will be fit when the visualizer is fit, otherwise, the\n",
    "        estimator will not be modified. If 'auto' (default), a helper method\n",
    "        will check if the estimator is fitted before fitting it again.\n",
    "\n",
    "    kwargs : dict\n",
    "        Keyword arguments that are passed to the base class and may influence\n",
    "        the visualization as defined in other Visualizers.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    silhouette_score_ : float\n",
    "        Mean Silhouette Coefficient for all samples. Computed via scikit-learn\n",
    "        `sklearn.metrics.silhouette_score`.\n",
    "\n",
    "    silhouette_samples_ : array, shape = [n_samples]\n",
    "        Silhouette Coefficient for each samples. Computed via scikit-learn\n",
    "        `sklearn.metrics.silhouette_samples`.\n",
    "\n",
    "    n_samples_ : integer\n",
    "        Number of total samples in the dataset (X.shape[0])\n",
    "\n",
    "    n_clusters_ : integer\n",
    "        Number of clusters (e.g. n_clusters or k value) passed to internal\n",
    "        scikit-learn model.\n",
    "\n",
    "    y_tick_pos_ : array of shape (n_clusters,)\n",
    "        The computed center positions of each cluster on the y-axis\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    >>> from yellowbrick.cluster import SilhouetteVisualizer\n",
    "    >>> from sklearn.cluster import KMeans\n",
    "    >>> model = db_SilhouetteVisualizer(KMeans(10))\n",
    "    >>> model.fit(X)\n",
    "    >>> model.show()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimator, ax=None, colors=None, is_fitted=\"auto\", **kwargs):\n",
    "\n",
    "        # Initialize the visualizer bases\n",
    "        super(db_SilhouetteVisualizer, self).__init__(estimator, ax=ax, **kwargs)\n",
    "\n",
    "        # Visual Properties\n",
    "        # Use colors if it is given, otherwise attempt to use colormap which\n",
    "        # which will override colors. If neither is found, default to None.\n",
    "        # The colormap may yet still be found in resolve_colors\n",
    "        self.colors = colors\n",
    "        if \"colormap\" in kwargs:\n",
    "            self.colors = kwargs[\"colormap\"]\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Fits the model and generates the silhouette visualization.\n",
    "        \"\"\"\n",
    "        # TODO: decide to use this method or the score method to draw.\n",
    "        # NOTE: Probably this would be better in score, but the standard score\n",
    "        # is a little different and I'm not sure how it's used.\n",
    "\n",
    "        if not check_fitted(self.estimator, is_fitted_by=self.is_fitted):\n",
    "            # Fit the wrapped estimator\n",
    "            self.estimator.fit(X, y, **kwargs)\n",
    "\n",
    "        # Get the properties of the dataset\n",
    "        self.n_samples_ = X.shape[0]\n",
    "        self.n_clusters_ = np.unique(self.estimator.labels_).shape[0]\n",
    "\n",
    "        # Compute the scores of the cluster\n",
    "        labels = self.estimator.fit_predict(X)\n",
    "        self.silhouette_score_ = silhouette_score(X, labels)\n",
    "        self.silhouette_samples_ = silhouette_samples(X, labels)\n",
    "\n",
    "        # Draw the silhouette figure\n",
    "        self.draw(labels)\n",
    "\n",
    "        # Return the estimator\n",
    "        return self\n",
    "\n",
    "\n",
    "    def draw(self, labels):\n",
    "        \"\"\"\n",
    "        Draw the silhouettes for each sample and the average score.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        labels : array-like\n",
    "            An array with the cluster label for each silhouette sample,\n",
    "            usually computed with ``predict()``. Labels are not stored on the\n",
    "            visualizer so that the figure can be redrawn with new data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Track the positions of the lines being drawn\n",
    "        y_lower = 10  # The bottom of the silhouette\n",
    "\n",
    "        # Get the colors from the various properties\n",
    "        color_kwargs = {\"n_colors\": self.n_clusters_}\n",
    "\n",
    "        if self.colors is None:\n",
    "            color_kwargs[\"colormap\"] = \"Set1\"\n",
    "        elif isinstance(self.colors, str):\n",
    "            color_kwargs[\"colormap\"] = self.colors\n",
    "        else:\n",
    "            color_kwargs[\"colors\"] = self.colors\n",
    "\n",
    "        colors = resolve_colors(**color_kwargs)\n",
    "\n",
    "        # For each cluster, plot the silhouette scores\n",
    "        self.y_tick_pos_ = []\n",
    "        \n",
    "        \n",
    "        for idx in range(-1, self.n_clusters_):\n",
    "\n",
    "            # Collect silhouette scores for samples in the current cluster\n",
    "            values = self.silhouette_samples_[labels == idx]\n",
    "            values.sort()\n",
    "\n",
    "            # Compute the size of the cluster and find upper limit\n",
    "            size = values.shape[0]\n",
    "            y_upper = y_lower + size\n",
    "\n",
    "            color = colors[idx]\n",
    "            self.ax.fill_betweenx(\n",
    "                np.arange(y_lower, y_upper),\n",
    "                0,\n",
    "                values,\n",
    "                facecolor=color,\n",
    "                edgecolor=color,\n",
    "                alpha=0.5,\n",
    "            )\n",
    "\n",
    "            # Collect the tick position for each cluster\n",
    "            self.y_tick_pos_.append(y_lower + 0.5 * size)\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        self.ax.axvline(\n",
    "            x=self.silhouette_score_,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            label=\"Average Silhouette Score\",\n",
    "        )\n",
    "\n",
    "        return self.ax\n",
    "\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        Prepare the figure for rendering by setting the title and adjusting\n",
    "        the limits on the axes, adding labels and a legend.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the title\n",
    "        self.set_title(\n",
    "            (\"Silhouette Plot of {} Clustering for {} Samples in {} Centers\").format(\n",
    "                self.name, self.n_samples_, self.n_clusters_\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Set the X and Y limits\n",
    "        # The silhouette coefficient can range from -1, 1;\n",
    "        # but here we scale the plot according to our visualizations\n",
    "\n",
    "        # l_xlim and u_xlim are lower and upper limits of the x-axis,\n",
    "        # set according to our calculated max and min score with necessary padding\n",
    "        l_xlim = max(-1, min(-0.1, round(min(self.silhouette_samples_) - 0.1, 1)))\n",
    "        u_xlim = min(1, round(max(self.silhouette_samples_) + 0.1, 1))\n",
    "        self.ax.set_xlim([l_xlim, u_xlim])\n",
    "\n",
    "        # The (n_clusters_+1)*10 is for inserting blank space between\n",
    "        # silhouette plots of individual clusters, to demarcate them clearly.\n",
    "        self.ax.set_ylim([0, self.n_samples_ + (self.n_clusters_ + 1) * 10])\n",
    "\n",
    "        # Set the x and y labels\n",
    "        self.ax.set_xlabel(\"silhouette coefficient values\")\n",
    "        self.ax.set_ylabel(\"cluster label\")\n",
    "\n",
    "        # Set the ticks on the axis object.\n",
    "        self.ax.set_yticks(self.y_tick_pos_)\n",
    "        self.ax.set_yticklabels(str(idx) for idx in range(self.n_clusters_))\n",
    "        # Set the ticks at multiples of 0.1\n",
    "        self.ax.xaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "\n",
    "        # Show legend (Average Silhouette Score axis)\n",
    "        self.ax.legend(loc=\"best\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_projected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-aa0122e525cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0my_db_pr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_projected\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_projected' is not defined"
     ]
    }
   ],
   "source": [
    "# list for range minimum sample size to be considered in hyperparameter tuning\n",
    "min_samples = list(np.arange(5, 20, 5))\n",
    "\n",
    "# list for range of \n",
    "eps = list(np.arange(0.15, 2.0, 0.5))\n",
    "\n",
    "y_db_pr = np.zeros((len(min_samples), X_projected.shape[0], len(eps)))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(min_samples), ncols=len(eps), figsize=(18, 12), sharex=True, sharey=True)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    for i, n in enumerate(min_samples):\n",
    "        for j, max_dist in enumerate(eps):\n",
    "            db = DBSCAN(eps=max_dist, min_samples=n)\n",
    "            y_db = db.fit_predict(X_projected)\n",
    "            y_db_pr[i, :, j] = y_agc\n",
    "            n_noise_ = len(db.labels_[db.labels_ == -1])\n",
    "            # Creating SilhouetteVisualizer and assigning it to an axis\n",
    "            visualizer = db_SilhouetteVisualizer(db, ax=axs[i, j], is_fitted=True, legend=True)\n",
    "            visualizer.fit(X_projected)\n",
    "            # Setting subplot legends via axis\n",
    "            axs[i, j].legend([f'{visualizer.n_clusters_} clusters, Score={visualizer.silhouette_score_:.3f}'],\n",
    "                                                loc=\"upper right\", frameon=True, fontsize=10)\n",
    "            # Setting subplot x and y labels\n",
    "            axs[i, j].set(xlabel='silhouette coefficient values', ylabel='cluster label')\n",
    "            # setting estimated number of noise points\n",
    "            \n",
    "            axs[i, j].text(-0.6, 300, '{} noise points'.format(n_noise_), ha='center', wrap=True)            \n",
    "            \n",
    "for i in range(len(eps)):\n",
    "    axs[0, i].set_title(\"eps = {}\".format(eps[i]))\n",
    "\n",
    "fig.text(0.5, 0.04, 'maximum distance', ha='center', fontsize=16)\n",
    "fig.text(0.04, 0.5, 'Minimum number of samples', va='center', rotation='vertical', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "- The highest score in the silhouette plots ($0.571$) shown above corresponds to $n$_$clusters$ = $6$, returned for the  hyperparameter combination $min$_$samples$ = $5$ and $eps$ = $1.15$. This score is significantly lower than the scores produced by the $K$-$Means$ and $AgglomerativeClustering$ algorithms ($0.713$). To make matters worse, it is also seen that the algorithm classifies more than half of the data as noise points for all combinations of $min$_$samples$ and $eps$.  \n",
    "\n",
    "- This quite surprising because it is expected that, by it's flexible and robust nature, such $Density$ $Based$ $Clustering$ algorithm should produce subtantially better results relative to the $Hierarchical$ and $Centroid-based$ $Clustering$ algorithms employed here. \n",
    "\n",
    "- This begs the question \"Why does the $DBSCAN$ algorithm perform so poorly in comparison?\"\n",
    "\n",
    "- The quality of DBSCAN relies on the distance measure used. The default distance metric used in the trained algorithm above is the Euclidean distance. As stated before, particularly for high-dimensional data, the Euclidean distance metric can be rendered almost useless--failing to converge--due to the $curse$ $of$ $dimensionality$. Since finding an appropriate value for $eps$ depends on this measure, it makes it tuning the hyperparameter $eps$ practically impossible.\n",
    "\n",
    "- This means that although, the other algorithm used here are plagued by this curse, this effect, is increasingly expressed in $DBSCAN$. \n",
    "\n",
    "- To verify this arguments, or perhaps remedy this, I approached the hyperparameter tuning from another perspective;\n",
    "\n",
    "    - According to literature in [J. Sander et al. Data Mining and Knowledge Discovery (1998)](https://link.springer.com/article/10.1023/A:1009745219419), If the data has more than 2 dimensions, $min$_$samples$ should be chosen to be = $2$$x$$dim$, where $dim$ = the dimensions of the data. Therefore, since the reduced feature space has $dim$ = 150, $min$_$samples$ is chosen to be = $300$\n",
    "    \n",
    "    - This satisfies the rule of thub that $min$_$samples$ should be greater than or equal to the dimensionality of the data set\n",
    "    \n",
    "    - Next, to determine the optimal $eps$, I adopted the technique described in [Nadia Rahmah and Imas Sukaesih Sitanggang 2016 IOP Conf. Ser.: Earth Environ. Sci. 31 012012](https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012). \n",
    "    \n",
    "    - This technique calculates the average distance between each point and its k nearest neighbors, where k = the $min$_$samples$ value selected, $300$. \n",
    "    \n",
    "    - The average k-distances are then plotted in ascending order on a k-distance graph. \n",
    "    \n",
    "    - The optimal value for $eps$ is the point of maximum curvature (obtained from the second derivative and first derivative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import NearestNeighbors from Scikit-learn to calculate the average distance between each point and its n_neighbors\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# obtained avarage distance between each point in the dataset and its 300 nearest neighbors.\n",
    "min_samples = 300 \n",
    "neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "neighbors_fit = neighbors.fit(X_projected)\n",
    "\n",
    "# obtain average distances\n",
    "distances, indices = neighbors_fit.kneighbors(X_projected)\n",
    "\n",
    "# Sort distance values by ascending value and plot\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)\n",
    "\n",
    "plt.ylabel('$Sorted$ $Average$ $Distance$')\n",
    "plt.xlabel('$Snapshots$ $Sorted$ $By$ $Distance$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eyeballing the plot above already shows that that there appears to be no clearly defined elbow for the sorted distances.\n",
    "\n",
    "- To obtain this, the point of maximum curvature which is approximated by taking the ratio of the second derivative and the square of the first derivative. This is implimented in the algorithm shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'distances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-32e1eeaa8957>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcurve\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcurve\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdistances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcurvature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'distances' is not defined"
     ]
    }
   ],
   "source": [
    "curve = []\n",
    "for i in range(0, 1001):\n",
    "    curve.append(1/(1+(distances[i-2] - distances[i-1])**2)**1.5)\n",
    "    \n",
    "curvature = []\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    curvature.append((distances[i-2] + distances[i] - 2*distances[i-1])/2 * curve[i-1])\n",
    "\n",
    "plt.plot(curvature)\n",
    "plt.ylabel('$Approximate$ $Curvature$')\n",
    "plt.xlabel('$Snapshots$ $Sorted$ $By$ $Distance$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The maximum value of this curve is equivalent to the optimal value for $eps$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.max(curvature)\n",
    "\n",
    "print(\"Optimal value for $eps$ is {}\".format(eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, I will again looping through combinations of $min$_$samples$ and $eps$ values slightly above ($min$_$samples$ = $350$, $eps$ = $8$) and below ($min$_$samples$ = $250$, $eps$ = $4$) the values estimated here to find the model of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_projected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ae54d865f477>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0my_db_pr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_projected\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_projected' is not defined"
     ]
    }
   ],
   "source": [
    "# list for range minimum sample size to be considered in hyperparameter tuning\n",
    "min_samples = list(np.arange(250, 400, 50))\n",
    "\n",
    "# list for range of \n",
    "eps = list(np.arange(4, 10, 2))\n",
    "\n",
    "y_db_pr = np.zeros((len(min_samples), X_projected.shape[0], len(eps)))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(min_samples), ncols=len(eps), figsize=(15, 15), sharex=True, sharey=True)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    for i, n in enumerate(min_samples):\n",
    "        for j, max_dist in enumerate(eps):\n",
    "            db = DBSCAN(eps=max_dist, min_samples=n)\n",
    "            y_db = db.fit_predict(X_projected)\n",
    "            y_db_pr[i, :, j] = y_agc\n",
    "            # Creating SilhouetteVisualizer and assigning it to an axis\n",
    "            visualizer = db_SilhouetteVisualizer(db, ax=axs[i, j], is_fitted=True, legend=True)\n",
    "            visualizer.fit(X_projected)\n",
    "            # Setting subplot legends via axis\n",
    "            axs[i, j].legend([f'{visualizer.n_clusters_} clusters, Score={visualizer.silhouette_score_:.3f}'],\n",
    "                                                loc=\"upper right\", frameon=True, fontsize=10)\n",
    "            # Setting subplot x and y labels\n",
    "            axs[i, j].set(xlabel='silhouette coefficient values', ylabel='cluster label')\n",
    "\n",
    "for i in range(len(eps)):\n",
    "    axs[0, i].set_title(\"eps = {}\".format(eps[i]))\n",
    "\n",
    "fig.text(0.5, 0.04, 'maximum distance', ha='center', fontsize=16)\n",
    "fig.text(0.04, 0.5, 'Minimum number of samples', va='center', rotation='vertical', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "- We can draw the following conlcusions from the silhouette analysis of the $Density$ $Based$ $Clustering$ algorthim;\n",
    "\n",
    "    - The silhouette coefficients do not marginally change for combinations of $min$_$samples$ and $eps$ values slightly above ($min$_$samples$ = $350$, $eps$ = $8$) and below ($min$_$samples$ = $250$, $eps$ = $4$).\n",
    "    \n",
    "    - This suggests that the technique applied here for obtaining the optimal values for $min$_$samples$ and $eps$ produces a good fit. Therefore, $min$_$samples$ is chosen to be $300$ and $eps$, $6.19$ for the tranined model.\n",
    "    \n",
    "    - There appears to be a striking similarity in the clustering (labelling) results and average silhouette coefficients ($0.647$) obtained here and those obtained for both $K$-$Means$ and $AgglomerativeClustering$ whith $n$_$clusters$ = $2$. \n",
    "    \n",
    "    - This is quite interesting because the initial combination of hyperparameters considered for $DBSCAN$ yielded no such results, validating the results obtained by the hyperparamter tuning techinique. \n",
    "    \n",
    "    - It is important to note that although this tuning techinique produces a better fit with $min$_$samples$ = $300$ and $eps$ = $6.19$, about half of the samples are in the cluster with below average silhouette score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitted Model\n",
    "\n",
    "- $DBSCAN$ fittted with $min$_$samples$ = $300$ and $eps$ = $6.19$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=6.19, min_samples=300)\n",
    "y_db = db.fit_predict(X_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "- Haven obtained the optimized hyperparameters for the three algorithms considered, Which of these models produces optimal clustering results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitted Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_projected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6b569ca5ecb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"k-means++\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_kmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_projected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0magc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maffinity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"euclidean\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"single\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_agc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_projected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_projected' is not defined"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3, init=\"k-means++\")\n",
    "y_kmeans = kmeans.fit_predict(X_projected)\n",
    "\n",
    "agc = AgglomerativeClustering(n_clusters=3, affinity=\"euclidean\", linkage=\"single\")\n",
    "y_agc = agc.fit_predict(X_projected)\n",
    "\n",
    "db = DBSCAN(eps=6.19, min_samples=300)\n",
    "y_db = db.fit_predict(X_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's visualize their silhouette plots side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_projected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2d76f5d0906f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Creating SilhouetteVisualizer for K-means\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvisualizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSilhouetteVisualizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_fitted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mvisualizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_projected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Setting subplot legends via axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     axs[0].legend([f'{visualizer.n_clusters_} clusters, Score={visualizer.silhouette_score_:.3f}'],\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_projected' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    # Creating SilhouetteVisualizer for K-means\n",
    "    visualizer = SilhouetteVisualizer(kmeans, ax=axs[0], is_fitted=True, legend=True)\n",
    "    visualizer.fit(X_projected)\n",
    "    # Setting subplot legends via axis\n",
    "    axs[0].legend([f'{visualizer.n_clusters_} clusters, Score={visualizer.silhouette_score_:.3f}'],\n",
    "                                                loc=\"upper right\", frameon=True, fontsize=10)\n",
    "    # Setting subplot x and y labels\n",
    "    axs[0].set(xlabel='silhouette coefficient values', ylabel='cluster label')\n",
    "    axs[0].set_title(\"Silhouette Plot for K-Means Clustering\", fontsize=10)\n",
    "\n",
    "    # Creating SilhouetteVisualizer for Agglomerative Clustering\n",
    "    visualizer = agc_SilhouetteVisualizer(agc, ax=axs[1], is_fitted=True, legend=True)\n",
    "    visualizer.fit(X_projected)\n",
    "    axs[1].legend([f'{visualizer.n_clusters_} clusters, Score={visualizer.silhouette_score_:.3f}'],\n",
    "                                                loc=\"upper right\", frameon=True, fontsize=10)\n",
    "    # Setting subplot x and y labels\n",
    "    axs[1].set(xlabel='silhouette coefficient values', ylabel='cluster label')\n",
    "    axs[1].set_title(\"Silhouette Plot for Agglomerative Clustering\", fontsize=10)\n",
    "\n",
    "    # Creating SilhouetteVisualizer for DBSCAN\n",
    "    visualizer = db_SilhouetteVisualizer(db, ax=axs[2], is_fitted=True, legend=True)\n",
    "    visualizer.fit(X_projected)\n",
    "    axs[2].legend([f'{visualizer.n_clusters_} clusters, Score={visualizer.silhouette_score_:.3f}'],\n",
    "                                                loc=\"upper right\", frameon=True, fontsize=10)\n",
    "    # Setting subplot x and y labels\n",
    "    axs[2].set(xlabel='silhouette coefficient values', ylabel='cluster label')\n",
    "    axs[2].set_title(\"Silhouette Plot for DBSCAN\", fontsize=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "- At first glance, it appears that both $K$-$Means$ and $AgglomerativeClustering$ produce the best results when the average silhouette coefficient alone is used as an assay, because they yield higher scores relative to $DBSCAN$. \n",
    "\n",
    "- However, it would be naive to completely rule out the $DBSCAN$ algorithm based on this alone. This is due to wide fluctuations in the thickness of the silhouette plots for the $K$-$Means$ and $AgglomerativeClustering$ algorithms predicting $n$_$clusters$ = $3$.  \n",
    "\n",
    "- Since the ground truth label assignments are known, the performs of all three clustering algorithms can further be evaluated using the following metrics;\n",
    "\n",
    "    - $Adjusted$ $Rand$ $Index$ ($ARI$):\n",
    "    \n",
    "        - This measures the similarity of the two data clusterings bouded below by 0.0 and above by 1.0\n",
    "        - Perfect labeling is scored 1.0\n",
    "        \n",
    "    - $Adjusted$ $Mutual$ $Information$ ($AMI$):\n",
    "    \n",
    "        - Similar to $ARI$, This measures agreement of the two data clusterings, ignoring permutations, bouded below by 0.0 and above by 1.0\n",
    "        - Perfect labeling is scored 1.0   \n",
    "        \n",
    "    - $Homogeneity$ and $Completeness$:\n",
    "    \n",
    "        - Homogeneity is a measure of the purity of each cluster. Checks if each cluster contains only members of a single class. This is boudned below by 0.0 and above by 1.0\n",
    "        - Completeness captures if all members of a given class are assigned to the same cluster. This is also boudned below by 0.0 and above by 1.0 \n",
    "        - Their harmonic mean called $V$-$measure$\n",
    "        \n",
    "    - Mathematical formalisms of these metrics can be found [here](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_on_data = pd.DataFrame(columns=[\"ARI\", \"AMI\", \"Homogeneity\", \"Completeness\", \"Silhouette\"], \n",
    "                                   index=[\"K-Means\", \"Agglomerative Clustering\", \"DBSCAN\"],\n",
    "                                   dtype=float)\n",
    "\n",
    "\n",
    "metrics_on_data.loc[\"K-Means\", \"ARI\"] = metrics.adjusted_rand_score(y_kmeans, flags)\n",
    "metrics_on_data.loc[\"K-Means\", \"AMI\"] = metrics.adjusted_mutual_info_score(y_kmeans, flags)\n",
    "metrics_on_data.loc[\"K-Means\", \"Homogeneity\"] = metrics.homogeneity_score(y_kmeans, flags)\n",
    "metrics_on_data.loc[\"K-Means\", \"Completeness\"] = metrics.completeness_score(y_kmeans, flags)\n",
    "metrics_on_data.loc[\"K-Means\", \"Silhouette\"] = metrics.silhouette_score(X_projected, y_kmeans, metric='euclidean')\n",
    "\n",
    "metrics_on_data.loc[\"Agglomerative Clustering\", \"ARI\"] = metrics.adjusted_rand_score(y_agc, flags)\n",
    "metrics_on_data.loc[\"Agglomerative Clustering\", \"AMI\"] = metrics.adjusted_mutual_info_score(y_agc, flags)\n",
    "metrics_on_data.loc[\"Agglomerative Clustering\", \"Homogeneity\"] = metrics.homogeneity_score(y_agc, flags)\n",
    "metrics_on_data.loc[\"Agglomerative Clustering\", \"Completeness\"] = metrics.completeness_score(y_agc, flags)\n",
    "metrics_on_data.loc[\"Agglomerative Clustering\", \"Silhouette\"] = metrics.silhouette_score(X_projected, y_agc, metric='euclidean')\n",
    "\n",
    "metrics_on_data.loc[\"DBSCAN\", \"ARI\"] = metrics.adjusted_rand_score(y_db, flags)\n",
    "metrics_on_data.loc[\"DBSCAN\", \"AMI\"] = metrics.adjusted_mutual_info_score(y_db, flags)\n",
    "metrics_on_data.loc[\"DBSCAN\", \"Homogeneity\"] = metrics.homogeneity_score(y_db, flags)\n",
    "metrics_on_data.loc[\"DBSCAN\", \"Completeness\"] = metrics.completeness_score(y_db, flags)\n",
    "metrics_on_data.loc[\"DBSCAN\", \"Silhouette\"] = metrics.silhouette_score(X_projected, y_db, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=metrics_on_data, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "- The heatmap shows that of the three algorithms, $DBSCAN$ has the highest scores ($approx$ $1$) for the $ARI$, $AMI$, $Homogeneity$ and $Completeness$ metrics and the lowest $Silhouette$ $Score$. \n",
    "\n",
    "- By virtue of their definition, this means that despite the comparatively lower $Silhouette$ $Score$, the $DBSCAN$ clustering algorithm, with $n$_$clusters$ = $2$, yields the predictions with the highest aggrements and similarity to the ground truth labels while maintining the purity of each cluster and ensuring that as many memebers of a given class are are assigned to the correct cluster for completeness. \n",
    "\n",
    "- Therefore, the optimal clustering model for this problem is the $DBSCAN$ algorithm with $min$_$samples$ = $300$ and $eps$ = $6.19$ which yields $n$_$clusters$ = $2$.\n",
    "\n",
    "- As seen in previous silhouette analysis, It is worth noting that similar results can be obtained with the $K$_$means$ and $Agglomerative$ $Clustering$ algortithms trained with $n$_$clusters$ = $2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
